{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eecf10ef-4103-4ee3-a99d-5fb159aaaa55",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e059b89-ead8-4733-82e2-23c9fc22f739",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    " \n",
    "\n",
    "# 步骤1: 数据预处理\n",
    "# 分割心跳信号序列为数值列表\n",
    "df['heartbeat_signals'] = df['heartbeat_signals'].apply(lambda x: [float(i) for i in x.split(',')])\n",
    "\n",
    "# 将列表转换为DataFrame的列\n",
    "max_length = max(df['heartbeat_signals'].apply(len))  # 找到最长序列的长度\n",
    "signals_df = pd.DataFrame(df['heartbeat_signals'].tolist()).fillna(0)  # 用0填充缺失值\n",
    "df = pd.concat([df, signals_df], axis=1)\n",
    "\n",
    "# 删除原始的heartbeat_signals列\n",
    "df = df.drop('heartbeat_signals', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781823eb-96be-4d7f-a0c1-11149ae6b1e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# 步骤3: 模型训练\n",
    "X = df.drop(['id', 'label'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# 使用SMOTE进行过采样处理类别不平衡，只对训练集进行处理\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76df4c46-47c9-4b36-b7f0-5c6eae6628d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abs-Sum Score: 0.04418875475363426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "abs_sum_scores = []  # 存储每一折的评分\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_resampled, y_test = y[train_index], y[test_index]\n",
    "    # 使用找到的最佳参数设置模型\n",
    "    best_params = {\n",
    "    'colsample_bytree': 0.7692681476866446,\n",
    "    'learning_rate': 0.0823076398078035,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 7,\n",
    "    'n_estimators': 527,\n",
    "    'subsample': 0.848553073033381,\n",
    "    'use_label_encoder': False,\n",
    "    'eval_metric': 'mlogloss'\n",
    "   }\n",
    "\n",
    "    # 初始化XGBoost模型\n",
    "    model = xgb.XGBClassifier(**best_params)\n",
    "    \n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # 预测概率\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # 计算abs-sum\n",
    "    # 首先，我们需要将y_test转换为one-hot编码形式，以匹配y_pred_proba的格式\n",
    "    y_test_one_hot = np.zeros((y_test.size, y_pred_proba.shape[1]))\n",
    "    y_test_one_hot[np.arange(y_test.size), y_test.astype(int)] = 1\n",
    "    \n",
    "    abs_sum = np.abs(y_test_one_hot - y_pred_proba).sum() / y_test.size\n",
    "    abs_sum_scores.append(abs_sum)\n",
    "\n",
    "# 计算平均abs-sum分数\n",
    "average_abs_sum = np.mean(abs_sum_scores)\n",
    "print(f\"Average Abs-Sum Score: {average_abs_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b48d9c-9105-4e8a-94d2-d4b302775fd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 28\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# 初始化XGBoost模型\u001B[39;00m\n\u001B[1;32m     26\u001B[0m model \u001B[38;5;241m=\u001B[39m xgb\u001B[38;5;241m.\u001B[39mXGBClassifier(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbest_params)\n\u001B[0;32m---> 28\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# 预测概率\u001B[39;00m\n\u001B[1;32m     31\u001B[0m y_pred_proba \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict_proba(X_test)\n",
      "File \u001B[0;32m/environment/miniconda3/lib/python3.10/site-packages/xgboost/core.py:730\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    728\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    729\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/environment/miniconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1519\u001B[0m, in \u001B[0;36mXGBClassifier.fit\u001B[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001B[0m\n\u001B[1;32m   1491\u001B[0m (\n\u001B[1;32m   1492\u001B[0m     model,\n\u001B[1;32m   1493\u001B[0m     metric,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1498\u001B[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001B[1;32m   1499\u001B[0m )\n\u001B[1;32m   1500\u001B[0m train_dmatrix, evals \u001B[38;5;241m=\u001B[39m _wrap_evaluation_matrices(\n\u001B[1;32m   1501\u001B[0m     missing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmissing,\n\u001B[1;32m   1502\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     feature_types\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_types,\n\u001B[1;32m   1517\u001B[0m )\n\u001B[0;32m-> 1519\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Booster \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1520\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1521\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1523\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1528\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective):\n\u001B[1;32m   1534\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m=\u001B[39m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjective\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/environment/miniconda3/lib/python3.10/site-packages/xgboost/core.py:730\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    728\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    729\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/environment/miniconda3/lib/python3.10/site-packages/xgboost/training.py:181\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mbefore_iteration(bst, i, dtrain, evals):\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 181\u001B[0m \u001B[43mbst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mafter_iteration(bst, i, dtrain, evals):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/environment/miniconda3/lib/python3.10/site-packages/xgboost/core.py:2051\u001B[0m, in \u001B[0;36mBooster.update\u001B[0;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[1;32m   2047\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_dmatrix_features(dtrain)\n\u001B[1;32m   2049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2050\u001B[0m     _check_call(\n\u001B[0;32m-> 2051\u001B[0m         \u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2052\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\n\u001B[1;32m   2053\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2054\u001B[0m     )\n\u001B[1;32m   2055\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2056\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dtrain, output_margin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "abs_sum_scores = []  # 存储每一折的评分\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # 使用找到的最佳参数设置模型\n",
    "    best_params = {\n",
    "    'colsample_bytree': 0.7692681476866446,\n",
    "    'learning_rate': 0.0823076398078035,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 7,\n",
    "    'n_estimators': 527,\n",
    "    'subsample': 0.848553073033381,\n",
    "    'use_label_encoder': False,\n",
    "    'eval_metric': 'mlogloss'\n",
    "   }\n",
    "\n",
    "    # 初始化XGBoost模型\n",
    "    model = xgb.XGBClassifier(**best_params)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 预测概率\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # 计算abs-sum\n",
    "    # 首先，我们需要将y_test转换为one-hot编码形式，以匹配y_pred_proba的格式\n",
    "    y_test_one_hot = np.zeros((y_test.size, y_pred_proba.shape[1]))\n",
    "    y_test_one_hot[np.arange(y_test.size), y_test.astype(int)] = 1\n",
    "    \n",
    "    abs_sum = np.abs(y_test_one_hot - y_pred_proba).sum() / y_test.size\n",
    "    abs_sum_scores.append(abs_sum)\n",
    "\n",
    "# 计算平均abs-sum分数\n",
    "average_abs_sum = np.mean(abs_sum_scores)\n",
    "print(f\"Average Abs-Sum Score: {average_abs_sum}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f2362e-284f-43be-b054-0227a16c2552",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting lightgbm\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ba/11/cb8b67f3cbdca05b59a032bb57963d4fe8c8d18c3870f30bed005b7f174d/lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m200.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /environment/miniconda3/lib/python3.10/site-packages (from lightgbm) (1.24.1)\n",
      "Requirement already satisfied: scipy in /environment/miniconda3/lib/python3.10/site-packages (from lightgbm) (1.11.3)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f95ba8-d11a-4298-9070-8b5061f16acc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Log Loss: 0.02038390054920139\n",
      "Average Abs-Sum: 0.035310493539996254\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "# 创建LightGBM数据集\n",
    "train_data = lgb.Dataset(X_resampled, label=y_resampled)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# 使用默认参数\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y_resampled)),\n",
    "    'metric': 'multi_logloss',\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "gbm = lgb.train(params, train_data, num_boost_round=100, valid_sets=[test_data])\n",
    "\n",
    "# 预测测试集\n",
    "y_pred_proba = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "# 计算测试集的对数损失\n",
    "log_loss_score = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Test Log Loss: {log_loss_score}\")\n",
    "\n",
    "# 首先，将实际标签转换为one-hot编码形式\n",
    "num_classes = np.unique(y_resampled).shape[0]  # 假设所有类别都出现在y_resampled中\n",
    "y_test_one_hot = np.zeros((y_test.shape[0], num_classes))\n",
    "y_test_one_hot[np.arange(y_test.shape[0]), y_test.astype(int)] = 1\n",
    "\n",
    "# 计算预测概率与实际标签之间的abs-sum\n",
    "abs_sum = np.sum(np.abs(y_pred_proba - y_test_one_hot)) / y_test.shape[0]\n",
    "\n",
    "print(f\"Average Abs-Sum: {abs_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b185ad6-f3bd-4209-82a5-0b03e581a22c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}